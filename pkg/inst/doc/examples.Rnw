%%\VignetteIndexEntry{hypergsplines: Hyper-g priors for GAM selection} 
%%\VignetteKeywords{penalised splines, Bayesian variable selection, g-prior, shrinkage}
%%\VignettePackage{hypergsplines}
%%\VignetteDepends{MASS,hypergsplines}

\documentclass[UKenglish]{scrreprt}

\usepackage[utf8]{inputenc}
\usepackage[UKenglish]{babel}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{url} 
\usepackage[sumlimits, intlimits, namelimits]{amsmath} % Mathe
\usepackage{amssymb}            % Symbole
\usepackage[caption = true]{subfig}
\usepackage[longnamesfirst,round]{natbib} 
\usepackage{tikz}  
\usepackage[nogin]{Sweave} 


\begin{document}

\title{Using the package \texttt{hypergsplines}:\\ some examples.}
\author{Daniel Saban\'es Bov\'e}
\maketitle

This short vignette shall introduce into the usage of the package
\texttt{hypergsplines}. For more information on the methodology, see the
technical report \citep{sabanesbove.etal2011}. 

If you have any questions or critique concerning the package, write an email to
me:
\href{mailto:daniel.sabanesbove@ifspm.uzh.ch}{\nolinkurl{daniel.sabanesbove@ifspm.uzh.ch}}. 
Many thanks in advance! 

\section{Pima Indians diabetes data}
\label{sec:pima-indi-diab}

First, we will have a look at the Pima Indians diabetes data, which is available
in the package \texttt{MASS}:

<<pima-data>>=
library(MASS)
pima <- rbind(Pima.tr, Pima.te)
pima$hasDiabetes <- as.numeric(pima$type == "Yes")
pima.nObs <- nrow(pima)
@ 

\paragraph{Setup}
\label{sec:setup}


For $n=\Sexpr{pima.nObs}$ women of Pima Indian heritage, seven possible
predictors for the presence of diabetes are recorded. We would like to
investigate with an additive logistic regression, which of them are relevant,
and what form the statistical association has -- is it a linear effect, or
rather a nonlinear effect? We will use a fully and automatic Bayesian approach
for this, see the technical report \citep{sabanesbove.etal2011} for more
details.

The first step is to define a \texttt{modelData} object, where we input the
response vector \texttt{y}, the matrix with the covariates, the spline type
(here we use \texttt{cubic} O'Sullivan splines with 4 inner knots) and the
exponential family (here a canonical \texttt{binomial} model, i.e. we want
logistic regression):

<<pima-setup>>=
library(hypergsplines)
modelData.pima <- with(pima,
                       glmModelData(y=hasDiabetes,
                                    X=
                                    cbind(npreg,
                                          glu,
                                          bp,
                                          skin,
                                          bmi,
                                          ped,
                                          age),
                                    splineType="cubic",
                                    nKnots=4L,                                    
                                    family=binomial))
@ 

\paragraph{Stochastic model search}
\label{sec:stoch-model-search}

<<pima-search-setup>>=
chainlength.pima <- 1000
computation.pima <- getComputation(useOpenMP=FALSE,
                                   higherOrderCorrection=FALSE)
@ 

Next, we will do a stochastic search on the (very large) model space to find
``good'' models. Here we have to decide on the model prior, and in this example
we use the \texttt{dependent} type which corrects for the implicit multiplicity
of testing. We use a \texttt{chainlength} of $\Sexpr{chainlength.pima}$, which
is rather small but enough for illustration purposes (usually one should use at
least $100\,000$), and save all models (in general \texttt{nModels} is the
number of models which are saved from all visited models). Finally, we decide
that we do not want to use OpenMP acceleration and no higher order correction
for the Laplace approximations. In order to be able to reproduce the analysis,
it is advisable to set a seed for the random number generator before starting
the stochastic search.

<<pima-search>>=
set.seed(93)
time.pima <-
    system.time(models.pima <-
                stochSearch(modelData=modelData.pima,
                            modelPrior="dependent",
                            chainlength=chainlength.pima,
                            nModels=chainlength.pima,
                            computation=computation.pima))
@ 

Wee see that the search took $\Sexpr{round(time.pima["elapsed"])}$~seconds, and
\Sexpr{models.pima[["numVisited"]]}~models were found. The ``models'' list
element of \texttt{models.pima} gives the table of the found models, with their
degrees of freedom for every covariate, the log marginal likelihood, the log
prior probability, the posterior probability and the number of times that the
sampler encountered that model:

<<pima-top-models>>=
head(models.pima$models)
map.pima <- models.pima$models[1, 1:7]
@ 
 
We have saved the degrees of freedom vector of the estimated MAP model in
\texttt{map.pima}.

\paragraph{Inclusion probabilities}
\label{sec:incl-prob}

The estimated marginal inclusion probabilities (probabilities for exclusion,
linear inclusion and nonlinear inclusion) for all covariates are also
saved: 

<<pima-incprobs>>=
round(models.pima$inclusionProbs,2)
@ 

\paragraph{Sampling model parameters}
\label{sec:sampl-model-param}

If we now want to look at the estimated covariate effects in the estimated MAP
model which has configuration (\Sexpr{paste(map.pima,collapse=",")}), then we
first need to generate parameter samples from that model:

<<pima-sampling>>=
mcmc.pima <- getMcmc(samples=5000L,
                     burnin=1000L,
                     step=1L,
                     nIwlsIterations=2L)

set.seed(634)
map.samples.pima <- glmGetSamples(config=map.pima,
                                  modelData=modelData.pima,
                                  mcmc=mcmc.pima,
                                  computation=computation.pima)
@ 

With the function \texttt{getMcmc}, we have defined a list of MCMC settings,
comprising the number of samples we would like to have in the end, the length of
the burn-in, the thinning step (here no thinning) and the number of IWLS
iterations used (with 2 steps you get a higher acceptance rate than with 1 step,
here the acceptance rate was
$\Sexpr{round(map.samples.pima$mcmc$acceptanceRatio,2)}$). The result
\texttt{map.samples.pima} has the following structure:

<<pima-samples-structure>>=
str(map.samples.pima)
@ 

It is a list with the \texttt{samples}, two diagnostics for the \texttt{mcmc},
and estimates for the log marginal likelihood (\texttt{logMargLik}). The latter
one contains the original ILA estimate, the MCMC estimate of the log marginal
likelihood with its standard error, and the coordinates of the posterior density
of $z=\log(g)$.

\paragraph{Curve estimates}
\label{sec:curve-estimates}


Now we can use the samples to plot the estimated effects of the MAP model
covariates, with the \texttt{plotCurveEstimate} function. For example:

<<pima-plot-ex, fig=TRUE>>=
plotCurveEstimate(covName="age",
                  samples=map.samples.pima$samples,
                  modelData=modelData.pima)
@ 

\paragraph{Post-processing}
\label{sec:post-processing}

If we want to have estimates of the degrees of freedom on a continuous scale
instead of the fixed grid $(0, 1, 2, 3, 4)$, we can optimise the marginal
likelihood with respect to the degrees of freedom of the MAP covariates:

<<pima-postprocess>>=
optim.map.pima <- postOptimize(modelData=modelData.pima,
                               modelConfig=map.pima,
                               computation=computation.pima)
optim.map.pima
@ 

For that model, we could again produce samples and plot curve estimates.

\paragraph{Prediction samples}
\label{sec:prediction-samples}

If we would like to get prediction samples for new covariate values, this is
also very easy via the \texttt{getFitSamples} function. Here we get posterior
predictive samples because we input a covariate matrix which is part of the
original covariate matrix used to fit the MAP model. Because the
\texttt{getFitSamples} function produces samples on the linear predictor scale,
we have to apply the appropriate response function (here the logistic
distribution function \texttt{plogis}) to get samples on the observation scale. 

<<pima-fit-samples>>=
fit.samples.pima <- getFitSamples(X=modelData.pima$origX[1:10,],
                                  samples=map.samples.pima$samples,
                                  modelData=modelData.pima)
obs.samples.pima <- plogis(fit.samples.pima)
@ 

The posterior predictive means are thus:

<<pima-postpred-means>>=
rowMeans(obs.samples.pima)
@ 

and could be compared to the actual observations

<<pima-observations>>=
modelData.pima$Y[1:10]
@ 


\paragraph{Model averaging}
\label{sec:model-averaging}

Model averaging works in principle similar to sampling from a single model, but
multiple model configurations are supplied and their respective log posterior
probabilities. For example, if we wanted to average the top ten models found, we
would do the following:

<<pima-model-averaging>>=
average.samples.pima <- 
    with(models.pima,
         getBmaSamples(config=models[1:10,],
                       logPostProbs=models$logMargLik[1:10] + 
                       models$logPrior[1:10],
                       nSamples=5000L,
                       modelData=modelData.pima,
                       mcmc=mcmc.pima,
                       computation=computation.pima))
@ 

Then internally, first the models are sampled, and for each sampled model so
many samples are drawn as determined by the model frequency in the model average
sample. On this sample object, the above presented functions can again be
applied (e.g. \texttt{plotCurveEstimate}).

To be continued \ldots

\bibliographystyle{abbrvnat}
\bibliography{examples}

\end{document}



%%% Local Variables: 
%%% mode: latex-math
%%% TeX-master: t
%%% coding: utf-8-unix
%%% ispell-local-dictionary: "british"
%%% End: 
